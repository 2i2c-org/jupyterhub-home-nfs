# Resource naming follows the z2jh pattern:
# - When fullnameOverride is a string (including ""), use it as the base name and append "home-nfs"
# - When fullnameOverride is not set, combine Release.Name with chart name
#
# Examples:
#   fullnameOverride: ""                          -> resources named "home-nfs"
#   fullnameOverride: "myjupyterhub"              -> resources named "myjupyterhub-home-nfs"
#   (no fullnameOverride)                         -> resources named "{{ .Release.Name }}-home-nfs"
#   (no fullnameOverride, nameOverride: "custom") -> resources named "{{ .Release.Name }}-custom-home-nfs"
nameOverride: ""
fullnameOverride: ""

# NFS Ganesha configuration
# NFS Ganesha is the user space NFS server that we use to serve the home directories

nfsServer:
  image:
    repository: ghcr.io/2i2c-org/nfs-ganesha
    tag: set-by-chartpress
  resources: {}
  # Whether to enable a client allow list
  # If true, only the clients in allowedClients will be allowed to access the NFS server
  # If false, all clients will be allowed to access the NFS server
  enableClientAllowlist: false
  # Clients allowed to access the NFS server
  # Only enforced if enableClientAllowlist is true
  # Example:
  # allowedClients:
  # - "10.128.0.0/20" # example CIDR block
  # - "10.120.*.1" # example IP pattern
  # - "*" # allow all clients
  allowedClients:
    - "127.0.0.1" # allow localhost

# Automatic filesystem resizer, so increases in underlying disk size
# are immediately reflected in available file system
autoResizer:
  enabled: true
  resources: {}

# Quota enforcer configuration
# This container enforces the quota on the home directories

quotaEnforcer:
  enabled: true
  image:
    repository: ghcr.io/2i2c-org/jupyterhub-home-nfs
    tag: set-by-chartpress


  # The "config" section below is rendered by a k8s Secret and mounted as a .yaml
  # file together with quota-enforcer-config.py for the jupyterhub-home-nfs
  # Python software to parse.
  #
  # Together they make the configured values below translate so that
  # config.QuotaManager.paths sets c.QuotaManager.paths, or more generally that
  # config.X.y sets c.X.y where X is a class and y is a configurable traitlet on
  # the class.
  config:
    QuotaManager:
      paths: ["/export"]
      hard_quota: 10
      wait_time: 30
      min_projid: 1000
      exclude: []
      quota_overrides: {}
      uid: 1000
      gid: 1000

  # The keys and values below are rendered by a k8s Secret and mounted as a .py
  # file together with quota-enforcer-config.py. These are then executed by the
  # quota enforcer container. They are useful for setting complex configurations
  # that are not easily expressed as a simple YAML structure.
  extraConfig: {}
    # Example:
    #
    # jupyterhub-home-nfs-01-config: |
    #   import os
    #   c.QuotaManager.paths = [os.environ["JUPYTERHUB_HOME_NFS_PATH"]]


  resources: {}

# Prometheus node exporter configuration
# We expose disk total usage + some other disk metrics with prometheus
# node exporter
nodeExporter:
  image:
    repository: quay.io/prometheus/node-exporter
    tag: v1.8.2
  resources: {}

# Persistent volume configuration

persistentVolume:
  # WARNING: Adding labels to existing PersistentVolumes may fail during upgrades
  # as PVs are largely immutable. Only enable this for new installations or if you
  # understand the risks. PersistentVolumeClaims receive standard labels by default
  # as they are more tolerant of updates.
  addStandardLabels: false
  # Setting this to 1M to make it obvious that this is not the real size.
  # The value can be changed to the actual size of the pre-provisioned disk to correctly
  # label the PV and PVC with the underlying disk size.
  # Not setting this to the actual size of the pre-provisioned disk doesn't affect the
  # functionality of the chart; only the labels on the PV and PVC will be incorrect.
  # The pre-provisioned disk ID is specified below in the cloud provider specific configuration
  size: 1M
  storageClass: ""
  accessModes:
    - ReadWriteOnce
  annotations: {}

service:
  type: ClusterIP

# Additional containers for the deployment
# `tpl` is applied when rendering these, so you can include {{ .Release.Name }}
# or references to the jupyterhub-home-nfs chart's configured values.
extraContainers: []

# Annotations for the deployment
annotations: {}

# Node selector for the deployment
nodeSelector: {}

# Affinity for the deployment
affinity: {}

# Tolerations for the deployment
tolerations: []

# Cloud provider specific configurations
eks:
  enabled: false
  volumeId: "your-eks-volume-id"

gke:
  enabled: false
  volumeId: "your-gke-volume-id"

openstack:
  enabled: false
  volumeId: "your-openstack-volume-id"
